{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dental Clinic RAG Chatbot: Interactive Customer Support System**\n",
    "\n",
    "Welcome to the **Dental Clinic RAG Chatbot**! This notebook demonstrates an AI-powered chatbot designed to assist customers with information about services, branches, and social media platforms available at our clinic. The chatbot is built using state-of-the-art natural language processing (NLP) techniques and integrates various data sources, including clinic services, branch locations, and social media platforms, to provide context-aware responses in both Arabic and English.\n",
    "\n",
    "## **Objective**\n",
    "The goal of this system is to create a seamless customer support experience by allowing users to ask questions related to:\n",
    "- **Clinic Services**: Detailed information about the treatments and services offered.\n",
    "- **Branches**: Locations and available branches of the clinic across the country.\n",
    "- **Social Media**: Platforms where customers can engage with the clinic and stay updated.\n",
    "\n",
    "## **How It Works**\n",
    "This chatbot leverages a combination of:\n",
    "1. **Sentence Embeddings**: Using pre-trained models for multilingual embeddings to understand and process customer queries.\n",
    "2. **Cosine Similarity**: To retrieve relevant documents and provide the most accurate responses based on the user’s query.\n",
    "3. **Language Models (LLM)**: A fine-tuned large language model (LLM) generates human-like responses, taking into account past conversations to ensure coherent and context-rich interactions.\n",
    "\n",
    "By running this notebook, you will be able to interact with the chatbot, ask questions, and receive accurate, dynamic responses based on real-time information from the clinic's database.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "1. **faiss-cpu** and **faiss-gpu**: Libraries for efficient similarity search and clustering of dense vectors, for use with CPU and GPU respectively.\n",
    "2. **transformers**: A library by Hugging Face for working with pre-trained models in natural language processing (NLP).\n",
    "3. **accelerate**: A library for optimized model training and deployment with multi-GPU support.\n",
    "4. **torch**: PyTorch, an open-source machine learning library for neural networks and deep learning.\n",
    "5. **bitsandbytes**: A lightweight library for fast training and inference of large-scale models.\n",
    "6. **langchain_community**: A community-driven library for building applications using large language models (LLMs).\n",
    "7. **langchain-huggingface**: An extension for integrating Hugging Face models with Langchain for NLP tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e9f47a1c-17fd-4c18-8a5d-4d1ad7344ff2",
    "_uuid": "2afd4909-d574-4e09-aba1-cb1d37b4ce17",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q faiss-cpu\n",
    "!pip install -q faiss-gpu\n",
    "!pip install -q transformers accelerate torch\n",
    "!pip install -q --upgrade transformers\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q torch --upgrade\n",
    "!pip install -q langchain_community\n",
    "!pip install -q -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "This code imports various libraries for NLP, deep learning, and database management. These libraries will be useful for building chatbots, working with pre-trained models, and performing vector-based similarity tasks.\n",
    "\n",
    "\n",
    "### Explanation of Libraries:\n",
    "1. **langchain**: A framework for building applications using language models (LLMs). It provides tools for prompts, memory, and chains that make it easier to develop chatbots.\n",
    "   - `PromptTemplate` is used to define the structure of the input to a language model.\n",
    "   - `LLMChain` is a class that chains together a prompt template and a model.\n",
    "   - `ConversationBufferMemory` stores the conversation history.\n",
    "\n",
    "2. **langchain_huggingface**: An extension to Langchain that allows easy integration with Hugging Face models for building conversational agents.\n",
    "   \n",
    "3. **transformers**: A Hugging Face library that provides access to pre-trained transformer models for NLP tasks, such as text generation.\n",
    "   - `pipeline` allows for easy usage of pre-trained models for tasks like text generation.\n",
    "   - `AutoTokenizer` and `AutoModelForCausalLM` are used to load tokenizer and causal language models.\n",
    "\n",
    "4. **sentence_transformers**: A library for generating sentence embeddings and computing similarity between sentences.\n",
    "   - `SentenceTransformer` helps in encoding sentences into vector representations.\n",
    "   - `cosine_similarity` from `sklearn` is used to compute similarity between sentence embeddings.\n",
    "\n",
    "5. **huggingface_hub**: A library to interact with the Hugging Face model hub, such as logging into the hub to access models.\n",
    "   \n",
    "6. **sqlite3**: A lightweight database library for storing and querying data in SQLite databases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6709d94d-4a1e-444f-a25b-b232609f7b35",
    "_uuid": "f50c3cd7-e9ad-4ea1-b7e1-fe2e639de687",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-27T18:41:42.295152Z",
     "iopub.status.busy": "2025-01-27T18:41:42.294851Z",
     "iopub.status.idle": "2025-01-27T18:41:56.294650Z",
     "shell.execute_reply": "2025-01-27T18:41:56.293636Z",
     "shell.execute_reply.started": "2025-01-27T18:41:42.295128Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "1. **Hugging Face Hub Login**: \n",
    "   - `login(\"your_token_access\")` logs into the Hugging Face hub using the provided authentication token. This step allows you to access and use models hosted on the Hugging Face platform.\n",
    "\n",
    "2. **Database Path**: \n",
    "   - `DATABASE_PATH = \"/kaggle/input/dental-clinic-rag-chatbot/ara_database.sqlite\"` specifies the path to the SQLite database where you likely store information related to the chatbot (e.g., services, branches, and social media data for your dental clinic chatbot).\n",
    "\n",
    "3. **Embedding Model**: \n",
    "   - `EMBEDDING_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"` initializes the embedding model. This specific model, **paraphrase-multilingual-MiniLM-L12-v2**, is designed to work with multilingual text and is used for generating sentence embeddings, which can be used for semantic search or similarity tasks.\n",
    "\n",
    "4. **Model ID for Text Generation**: \n",
    "   - `MODEL_ID = \"meta-llama/Llama-3.1-8B\"` sets up the pre-trained **Llama-3.1-8B** model from Meta, which is a large language model used for tasks like text generation, Q&A, and chatbot interactions. This model is capable of handling complex natural language tasks.\n",
    "\n",
    "This section sets up all necessary configurations for integrating Hugging Face models, preparing the embedding model, and pointing to the relevant database for storing information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb3eb374-1b80-4c2a-a0c4-2960d93ba33d",
    "_uuid": "b573c2dc-aebb-4f60-9750-093bda918e32",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-27T18:42:10.313035Z",
     "iopub.status.busy": "2025-01-27T18:42:10.312228Z",
     "iopub.status.idle": "2025-01-27T18:42:10.490615Z",
     "shell.execute_reply": "2025-01-27T18:42:10.489768Z",
     "shell.execute_reply.started": "2025-01-27T18:42:10.312994Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Logging in to Hugging Face Hub\n",
    "login(\"you_token_access\")\n",
    "\n",
    "# Define the path to the database and embedding model\n",
    "DATABASE_PATH = \"/kaggle/input/dental-clinic-rag-chatbot/ara_database.sqlite\"\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "# MODEL_ID = \"silma-ai/SILMA-9B-Instruct-v1.0\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Quantization and Loading\n",
    "\n",
    "This section of the code configures model quantization, loads the tokenizer, and loads the model with the defined quantization configuration. \n",
    "\n",
    "1. **Quantization Configuration**:\n",
    "   - `quantization_config = BitsAndBytesConfig(...)`:\n",
    "     - `load_in_8bit=True`: This flag enables 8-bit quantization for the model. Quantization reduces the model's memory usage by using lower-precision data types (e.g., 8-bit integers instead of 32-bit floats) while maintaining model performance.\n",
    "     - `llm_int8_threshold=6.0`: This optional parameter sets a threshold for dynamic quantization. It allows the model to adjust how quantization is applied based on the size of the model layers.\n",
    "     - `llm_int8_skip_modules=None`: This optional parameter allows specifying modules that should be skipped during quantization. By leaving it as `None`, all layers of the model will be quantized.\n",
    "\n",
    "2. **Loading the Tokenizer**:\n",
    "   - `tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)`:\n",
    "     - Loads the tokenizer for the specified pre-trained model (`MODEL_ID`). Tokenizers are used to convert input text into token IDs that the model can process, and vice versa.\n",
    "\n",
    "3. **Loading the Model**:\n",
    "   - `model = AutoModelForCausalLM.from_pretrained(...)`:\n",
    "     - This command loads the pre-trained causal language model (`MODEL_ID`) using the quantization configuration. \n",
    "     - The model is loaded onto the appropriate device (either GPU or CPU) based on the `device_map=\"auto\"` parameter.\n",
    "     - The quantization configuration applied during model loading ensures that the model's memory footprint is reduced while maintaining performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19b102e3-382d-469f-b1c7-972bd8a1e963",
    "_uuid": "8c98e1c9-a651-479d-ac00-c33d7b03e8e4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-27T18:42:11.387421Z",
     "iopub.status.busy": "2025-01-27T18:42:11.387137Z",
     "iopub.status.idle": "2025-01-27T18:43:25.427980Z",
     "shell.execute_reply": "2025-01-27T18:43:25.427118Z",
     "shell.execute_reply.started": "2025-01-27T18:42:11.387399Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    llm_int8_threshold=6.0,  # Optional: threshold for dynamic quantization\n",
    "    llm_int8_skip_modules=None  # Optional: specify modules to skip during quantization\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Load model with quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"  # Automatically map to GPU/CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation Pipeline and LangChain Integration\n",
    "\n",
    "This section of the code sets up a text generation pipeline using a pre-trained model and tokenizer, and then wraps the pipeline for integration with LangChain.\n",
    "\n",
    "1. **Setting the Padding Token**:\n",
    "   - `tokenizer.pad_token_id = tokenizer.eos_token_id`:\n",
    "     - This line ensures that the padding token (`pad_token_id`) is set to the same token as the end-of-sequence token (`eos_token_id`). This is necessary because the model may not have a separate padding token, and setting the padding token to the end-of-sequence token ensures that padding is handled correctly.\n",
    "\n",
    "2. **Creating the Text Generation Pipeline**:\n",
    "   - `text_gen_pipeline = pipeline(...)`:\n",
    "     - This command sets up a text generation pipeline using the pre-trained model (`model`) and tokenizer (`tokenizer`). The pipeline is configured with the following parameters:\n",
    "       - `do_sample=True`: This enables sampling during text generation, meaning that the model will generate diverse outputs based on probability distributions.\n",
    "       - `temperature=0.5`: Controls the randomness of the generation. A lower temperature (closer to 0) results in more deterministic outputs, while higher values (closer to 1) generate more diverse outputs.\n",
    "       - `top_p=0.65`: This parameter uses nucleus sampling, where the model considers only the top `p` most probable next tokens, summing up to a probability of `p` (in this case, 65%).\n",
    "       - `max_new_tokens=256`: Limits the maximum number of tokens the model can generate in one response to 256.\n",
    "       - `repetition_penalty=1.2`: This reduces the likelihood of the model repeating phrases or generating repetitive text.\n",
    "\n",
    "3. **Wrapping the Pipeline for LangChain**:\n",
    "   - `llm = HuggingFacePipeline(pipeline=text_gen_pipeline)`:\n",
    "     - This line wraps the text generation pipeline for use with LangChain. The `HuggingFacePipeline` class allows LangChain to interface with the pipeline, enabling you to use it in larger, more complex applications such as chatbots or interactive systems.\n",
    "\n",
    "This setup allows you to generate text responses based on the input, which can be integrated into a broader conversational system using LangChain.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:08:45.013486Z",
     "iopub.status.busy": "2025-01-27T20:08:45.013156Z",
     "iopub.status.idle": "2025-01-27T20:08:45.019931Z",
     "shell.execute_reply": "2025-01-27T20:08:45.019251Z",
     "shell.execute_reply.started": "2025-01-27T20:08:45.013458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Create a pipeline\n",
    "text_gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.65,\n",
    "    max_new_tokens=256,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "# Wrap the pipeline for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=text_gen_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Embedding Model\n",
    "\n",
    "1. **Loading the Sentence Transformer Model**:\n",
    "   - `embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)`:\n",
    "     - This line loads a pre-trained embedding model from the `sentence-transformers` library. The model specified by `EMBEDDING_MODEL_NAME` (in this case, `\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"`) is designed to generate sentence embeddings. These embeddings are vector representations of sentences that capture semantic meaning in a dense format, allowing you to compare and analyze text based on their meanings.\n",
    "\n",
    "   - The `SentenceTransformer` class is specifically designed to handle tasks such as:\n",
    "     - Text similarity: Comparing sentences or documents to find how similar they are.\n",
    "     - Text clustering: Grouping sentences or documents with similar meanings.\n",
    "     - Information retrieval: Finding the most relevant documents or answers based on the input query.\n",
    "\n",
    "   - By using this embedding model, you can convert your input text into fixed-size vectors (embeddings) that can then be used for tasks like semantic search, document retrieval, and various NLP-based applications.\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b4cb2cad-5d77-43d5-b9b1-4aafc3a4545b",
    "_uuid": "20044d62-b31a-4356-833a-daebc6a65d17",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-27T18:43:49.991340Z",
     "iopub.status.busy": "2025-01-27T18:43:49.991014Z",
     "iopub.status.idle": "2025-01-27T18:43:51.998071Z",
     "shell.execute_reply": "2025-01-27T18:43:51.997077Z",
     "shell.execute_reply.started": "2025-01-27T18:43:49.991314Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the SQLite Database and Reading Data\n",
    "\n",
    "1. **Connecting to the SQLite Database**:\n",
    "   - `conn = sqlite3.connect(DATABASE_PATH)`:\n",
    "     - This line establishes a connection to the SQLite database using the provided `DATABASE_PATH`. The connection object `conn` allows you to interact with the database (in this case, located at the specified path, i.e., `\"/kaggle/input/dental-clinic-rag-chatbot/ara_database.sqlite\"`).\n",
    "\n",
    "2. **Reading Data from the SQLite Database**:\n",
    "   - The following queries use `pd.read_sql_query()` to read data from each table in the SQLite database and load it into pandas DataFrames:\n",
    "     - `services_arabic_df = pd.read_sql_query(\"SELECT * FROM Services_Arabic\", conn)`:\n",
    "       - This retrieves all records from the `Services_Arabic` table and stores them in the `services_arabic_df` DataFrame.\n",
    "     - `branches_arabic_df = pd.read_sql_query(\"SELECT * FROM Branches_Arabic\", conn)`:\n",
    "       - This retrieves all records from the `Branches_Arabic` table and stores them in the `branches_arabic_df` DataFrame.\n",
    "     - `socialmedia_arabic_df = pd.read_sql_query(\"SELECT * FROM SocialMedia_Arabic\", conn)`:\n",
    "       - This retrieves all records from the `SocialMedia_Arabic` table and stores them in the `socialmedia_arabic_df` DataFrame.\n",
    "     - `services_english_df = pd.read_sql_query(\"SELECT * FROM Services_English\", conn)`:\n",
    "       - This retrieves all records from the `Services_English` table and stores them in the `services_english_df` DataFrame.\n",
    "     - `branches_english_df = pd.read_sql_query(\"SELECT * FROM Branches_English\", conn)`:\n",
    "       - This retrieves all records from the `Branches_English` table and stores them in the `branches_english_df` DataFrame.\n",
    "     - `socialmedia_english_df = pd.read_sql_query(\"SELECT * FROM SocialMedia_English\", conn)`:\n",
    "       - This retrieves all records from the `SocialMedia_English` table and stores them in the `socialmedia_english_df` DataFrame.\n",
    "\n",
    "3. **Closing the Database Connection**:\n",
    "   - `conn.close()`:\n",
    "     - Once all the necessary data is fetched into pandas DataFrames, the connection to the SQLite database is closed to free up resources.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T18:43:51.999417Z",
     "iopub.status.busy": "2025-01-27T18:43:51.999161Z",
     "iopub.status.idle": "2025-01-27T18:43:52.056208Z",
     "shell.execute_reply": "2025-01-27T18:43:52.055293Z",
     "shell.execute_reply.started": "2025-01-27T18:43:51.999394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "# Read each table into a pandas DataFrame\n",
    "services_arabic_df = pd.read_sql_query(\"SELECT * FROM Services_Arabic\", conn)\n",
    "branches_arabic_df = pd.read_sql_query(\"SELECT * FROM Branches_Arabic\", conn)\n",
    "socialmedia_arabic_df = pd.read_sql_query(\"SELECT * FROM SocialMedia_Arabic\", conn)\n",
    "services_english_df = pd.read_sql_query(\"SELECT * FROM Services_English\", conn)\n",
    "branches_english_df = pd.read_sql_query(\"SELECT * FROM Branches_English\", conn)\n",
    "socialmedia_english_df = pd.read_sql_query(\"SELECT * FROM SocialMedia_English\", conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Arabic and English DataFrames\n",
    "\n",
    "1. **Services DataFrame**:\n",
    "   - `services_df = pd.concat([services_arabic_df, services_english_df], keys=['Arabic', 'English'], names=['Language'])`:\n",
    "     - This code combines the Arabic and English `Services` DataFrames (`services_arabic_df` and `services_english_df`) into a single DataFrame. \n",
    "     - The `keys=['Arabic', 'English']` argument adds a new level to the index, indicating the language of each entry (Arabic or English).\n",
    "     - The `names=['Language']` argument assigns a name to this new index level, making it clear that this index represents the language.\n",
    "\n",
    "2. **Branches DataFrame**:\n",
    "   - `branches_df = pd.concat([branches_arabic_df, branches_english_df], keys=['Arabic', 'English'], names=['Language'])`:\n",
    "     - This combines the Arabic and English `Branches` DataFrames (`branches_arabic_df` and `branches_english_df`) in a similar way as done for the services.\n",
    "     - It also adds a `Language` index to differentiate between Arabic and English branches.\n",
    "\n",
    "3. **Social Media DataFrame**:\n",
    "   - `social_media_df = pd.concat([socialmedia_arabic_df, socialmedia_english_df], keys=['Arabic', 'English'], names=['Language'])`:\n",
    "     - This combines the Arabic and English `SocialMedia` DataFrames (`socialmedia_arabic_df` and `socialmedia_english_df`), creating a unified DataFrame with an additional language index.\n",
    "     - It follows the same approach as the previous two, allowing you to track the language of each social media entry.\n",
    "\n",
    "### Summary\n",
    "- **`pd.concat()`** is used to concatenate DataFrames along rows, with a hierarchical index created to indicate which entries are in Arabic and which are in English.\n",
    "- This results in three DataFrames (`services_df`, `branches_df`, `social_media_df`), each containing both Arabic and English data, with the language clearly indicated in the index.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T18:43:52.057694Z",
     "iopub.status.busy": "2025-01-27T18:43:52.057409Z",
     "iopub.status.idle": "2025-01-27T18:43:52.064323Z",
     "shell.execute_reply": "2025-01-27T18:43:52.063296Z",
     "shell.execute_reply.started": "2025-01-27T18:43:52.057664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Services dataframe combining Arabic and English services\n",
    "services_df = pd.concat([services_arabic_df, services_english_df], keys=['Arabic', 'English'], names=['Language'])\n",
    "\n",
    "# Branches dataframe combining Arabic and English branches\n",
    "branches_df = pd.concat([branches_arabic_df, branches_english_df], keys=['Arabic', 'English'], names=['Language'])\n",
    "\n",
    "# Social Media dataframe combining Arabic and English social media\n",
    "social_media_df = pd.concat([socialmedia_arabic_df, socialmedia_english_df], keys=['Arabic', 'English'], names=['Language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Retrieval Function Using Cosine Similarity\n",
    "\n",
    "The `retrieve_relevant_documents` function retrieves the most relevant documents from three different categories (services, branches, and social media) based on a query, using cosine similarity and sentence embeddings.\n",
    "\n",
    "#### Steps Involved:\n",
    "1. **Query Embedding**:\n",
    "   - The function first encodes the query using the `embedding_model` (in this case, a sentence transformer model). This converts the query text into a numerical vector (embedding) that can be compared to the embeddings of other documents.\n",
    "\n",
    "\n",
    "2. **Document Embeddings**:\n",
    "   - The function then encodes the documents in each of the three categories (services, branches, and social media). These documents are converted into embeddings using the same model.\n",
    "\n",
    "\n",
    "3. **Cosine Similarity Calculation**:\n",
    "   - The query embedding is compared to each of the document embeddings using **cosine similarity**. Cosine similarity measures the angle between two vectors, with a smaller angle indicating higher similarity.\n",
    "\n",
    "\n",
    "4. **Retrieve Top-K Most Relevant Documents**:\n",
    "   - For each category (services, branches, and social media), the function selects the top `k` most similar documents based on their cosine similarity scores. The documents with the highest similarity scores are chosen.\n",
    "\n",
    "\n",
    "5. **Return Relevant Documents**:\n",
    "   - The function returns a dictionary containing the top `k` relevant documents from each category. The keys in the dictionary are the names of the categories (\"services\", \"branches\", and \"social_media\").\n",
    "\n",
    "\n",
    "#### Example Usage:\n",
    "- Given a user query, the function will return the top 10 most relevant services, branches, and social media entries based on the query.\n",
    "- You can adjust the `top_k` parameter to control how many documents are returned from each category.\n",
    "\n",
    "### Summary:\n",
    "This function uses cosine similarity to compare a user’s query against a set of documents across three categories. It retrieves and returns the top `k` most relevant documents for each category based on the similarity scores.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0f714b09-ad11-4564-818c-0995bb4e0835",
    "_uuid": "25c6db4d-c48d-44d3-bae2-e399230a1d94",
    "execution": {
     "iopub.execute_input": "2025-01-27T18:43:52.290245Z",
     "iopub.status.busy": "2025-01-27T18:43:52.289999Z",
     "iopub.status.idle": "2025-01-27T18:43:52.295516Z",
     "shell.execute_reply": "2025-01-27T18:43:52.294740Z",
     "shell.execute_reply.started": "2025-01-27T18:43:52.290225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Simple retrieval function using cosine similarity\n",
    "def retrieve_relevant_documents(query, services_df, branches_df, social_media_df, top_k=10):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Encode documents in all dataframes\n",
    "    services_embeddings = embedding_model.encode(services_df['service_name'].tolist())\n",
    "    branches_embeddings = embedding_model.encode(branches_df['branch_name'].tolist())\n",
    "    social_media_embeddings = embedding_model.encode(social_media_df['platform_name'].tolist())\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    service_similarities = cosine_similarity(query_embedding, services_embeddings)\n",
    "    branch_similarities = cosine_similarity(query_embedding, branches_embeddings)\n",
    "    social_media_similarities = cosine_similarity(query_embedding, social_media_embeddings)\n",
    "\n",
    "    # Get top k most similar documents for each category\n",
    "    top_services = services_df.iloc[service_similarities.argsort()[0][-top_k:][::-1]]\n",
    "    top_branches = branches_df.iloc[branch_similarities.argsort()[0][-top_k:][::-1]]\n",
    "    top_social_media = social_media_df.iloc[social_media_similarities.argsort()[0][-top_k:][::-1]]\n",
    "\n",
    "    # Combine the results from services, branches, and social media\n",
    "    relevant_docs = {\n",
    "        \"services\": top_services,\n",
    "        \"branches\": top_branches,\n",
    "        \"social_media\": top_social_media\n",
    "    }\n",
    "\n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot Response Generation Function\n",
    "\n",
    "The `get_chatbot_response` function processes a user's query and generates a relevant response by retrieving documents from a set of predefined dataframes (services, branches, and social media) and utilizing a language model for generating the final response.\n",
    "\n",
    "#### Steps Involved:\n",
    "\n",
    "1. **Retrieve Relevant Documents**:\n",
    "   - The function calls the `retrieve_relevant_documents` function to find the top `k` most relevant documents for the given query. These documents are retrieved from three categories: services, branches, and social media.\n",
    "\n",
    "\n",
    "2. **Extract Relevant Information**:\n",
    "   - After retrieving the relevant documents, the function extracts the necessary information from each category (service names, branch names, and social media platform names). These values are joined together into strings.\n",
    "\n",
    "\n",
    "3. **Create the Input Prompt**:\n",
    "   - The function creates a structured input prompt for the chatbot, including the relevant information about services, branches, and social media platforms. The prompt also contains the user's query.\n",
    "\n",
    "\n",
    "4. **Set Up Prompt and Memory**:\n",
    "   - The function defines a template that includes placeholders for the conversation history and the generated input prompt. It also initializes memory to store the conversation history.\n",
    "\n",
    "5. **Generate Response Using LLM Chain**:\n",
    "   - The language model (LLM) chain is initialized with the prompt template, the LLM model, and memory to generate a response. The model is invoked with the input prompt and the current chat history.\n",
    "\n",
    "\n",
    "6. **Handle the Response**:\n",
    "   - The response from the LLM is captured, and the assistant's message is extracted. The assistant's response is then saved in the conversation memory for future interactions.\n",
    "\n",
    "\n",
    "7. **Return the Assistant's Message**:\n",
    "   - Finally, the assistant's message (response) is returned to the user.\n",
    "\n",
    "\n",
    "8. **Error Handling**:\n",
    "   - If any error occurs during the process, a user-friendly error message is returned.\n",
    "\n",
    "\n",
    "#### Example Usage:\n",
    "- Given a user query, this function generates a relevant chatbot response by retrieving information from services, branches, and social media based on cosine similarity.\n",
    "- The conversation history is maintained to ensure the chatbot provides coherent and contextually aware responses across multiple turns.\n",
    "\n",
    "### Summary:\n",
    "This function processes a user's query, retrieves the most relevant documents from services, branches, and social media, and then uses an LLM (language model) to generate a chatbot response. The conversation is saved in memory, ensuring that the chatbot can handle multi-turn conversations efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:08:54.357922Z",
     "iopub.status.busy": "2025-01-27T20:08:54.357601Z",
     "iopub.status.idle": "2025-01-27T20:08:54.365417Z",
     "shell.execute_reply": "2025-01-27T20:08:54.364379Z",
     "shell.execute_reply.started": "2025-01-27T20:08:54.357897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_chatbot_response(query, services_df, branches_df, social_media_df, llm):\n",
    "    try:\n",
    "        # Retrieve relevant documents using the query and dataframes\n",
    "        relevant_docs = retrieve_relevant_documents(query, services_df, branches_df, social_media_df, top_k=3)\n",
    "\n",
    "        # Extract relevant information from the documents\n",
    "        services = \", \".join(relevant_docs[\"services\"][\"service_name\"].tolist())\n",
    "        branches = \", \".join(relevant_docs[\"branches\"][\"branch_name\"].tolist())\n",
    "        social_media = \", \".join(relevant_docs[\"social_media\"][\"platform_name\"].tolist())\n",
    "\n",
    "        # Generalized prompt template for a chatbot, including context\n",
    "        input_prompt = f\"\"\"\n",
    "        أنت روبوت دردشة في المملكة العربية السعودية. هدفي هو مساعدة المستخدمين في الحصول على إجابات لأسئلتهم حول خدماتنا المتوفرة وأي استفسار عام.\n",
    "\n",
    "        الخدمات المتوفرة في عيادتنا هي:\n",
    "        {services}\n",
    "\n",
    "        الفروع المتوفرة لدينا في المملكة هي:\n",
    "        {branches}\n",
    "\n",
    "        يمكنك التواصل معنا عبر منصات الوسائط الاجتماعية التالية:\n",
    "        {social_media}\n",
    "\n",
    "        السؤال: {query}\n",
    "        \"\"\"\n",
    "\n",
    "        # Create an updated prompt template to include chat history\n",
    "        template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
    "\n",
    "        \n",
    "        {input_prompt}<|end|>\n",
    "        <|assistant|>\"\"\"\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"input_prompt\", \"chat_history\"]\n",
    "        )\n",
    "\n",
    "        # Initialize memory to store and retrieve the conversation history\n",
    "        memory = ConversationBufferMemory(memory_key=\"chat_history\",)\n",
    "\n",
    "        # Chain the LLM, Prompt, and Memory together\n",
    "        llm_chain = LLMChain(\n",
    "            prompt=prompt,\n",
    "            llm=llm,\n",
    "            memory=memory\n",
    "        )\n",
    "\n",
    "        # Invoke the LLM model and capture the response\n",
    "        response = llm_chain.invoke({\n",
    "            \"input_prompt\": input_prompt,\n",
    "            \"chat_history\": memory.load_memory_variables({})[\"chat_history\"]\n",
    "        })\n",
    "\n",
    "        # Extract assistant's message from the response\n",
    "        assistant_message = response.get(\"text\", \"\").strip()\n",
    "\n",
    "        # Save the user query and assistant response to the conversation memory\n",
    "        memory.save_context(\n",
    "            inputs={\"query\": query},\n",
    "            outputs={\"response\": assistant_message}\n",
    "        )\n",
    "\n",
    "        # Return only the assistant's message\n",
    "        return assistant_message\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors and provide a user-friendly message\n",
    "        return f\"عذراً، حدث خطأ: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Welcome Message**: When the program starts, it prints a welcome message in Arabic asking the user how it can assist them. It also provides the option to exit the program by typing `'exit'`.\n",
    "\n",
    "2. **User Input**: The chatbot waits for the user to input a question. The user can type their query in Arabic, and the chatbot will process it.\n",
    "\n",
    "3. **Exit Condition**: If the user types `'exit'` (case-insensitive), the loop ends, and the program prints `\"إلى اللقاء!\"` (Goodbye in Arabic), before breaking out of the loop.\n",
    "\n",
    "4. **Get Chatbot Response**: If the user doesn't type `'exit'`, the program passes the user's query to the `get_chatbot_response` function, which processes it and generates an appropriate response using the LLM chain, relevant documents, and conversation history.\n",
    "\n",
    "5. **Print the Response**: The chatbot's response is printed to the console with the label `\"Robt:\"`.\n",
    "\n",
    "### How It Works:\n",
    "- The loop runs continuously, processing user input and generating chatbot responses until the user decides to exit by typing `'exit'`.\n",
    "- The function `get_chatbot_response` is used to generate responses based on the context of the conversation and the information available in the `services_df`, `branches_df`, and `social_media_df`.\n",
    "\n",
    "### Improvements/Additional Features:\n",
    "- **Better Exit Handling**: You could include additional phrases like `quit`, `exit`, or `bye` to handle user exit commands more flexibly.\n",
    "- **Contextual Conversations**: The chatbot currently relies on the `get_chatbot_response` function, which includes context from previous interactions. You can improve the conversational experience by adding more sophisticated handling of context or multiple dialogue turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:32:31.833143Z",
     "iopub.status.busy": "2025-01-27T20:32:31.832756Z",
     "iopub.status.idle": "2025-01-27T20:32:36.522855Z",
     "shell.execute_reply": "2025-01-27T20:32:36.522065Z",
     "shell.execute_reply.started": "2025-01-27T20:32:31.833112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"مرحبًا! كيف يمكنني مساعدتك؟ اكتب 'exit' للخروج.\")\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"سؤالك: \")\n",
    "\n",
    "        # Exit if the user types \"exit\"\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"إلى اللقاء!\")\n",
    "            break\n",
    "\n",
    "        # Get the chatbot response\n",
    "        assistant_message = get_chatbot_response(query=user_input,\n",
    "                                        services_df=services_df,\n",
    "                                        branches_df=branches_df,\n",
    "                                        social_media_df=social_media_df,\n",
    "                                        llm=llm)\n",
    "\n",
    "        # Print the response\n",
    "        print(\"Robt: \", assistant_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "\n",
    "In this notebook, we've demonstrated how to build an advanced, multilingual chatbot for a dental clinic using state-of-the-art natural language processing and machine learning techniques. By integrating multiple data sources, including clinic services, branch information, and social media platforms, the chatbot is able to provide personalized, context-aware responses to user queries.\n",
    "\n",
    "The system utilizes a combination of **sentence embeddings** for accurate query understanding, **cosine similarity** for retrieving the most relevant documents, and a **large language model** (LLM) to generate human-like responses. This ensures that users receive precise, meaningful information in both Arabic and English, making the chatbot a valuable tool for customer engagement and support.\n",
    "\n",
    "By running this system, the clinic can offer customers an intuitive, automated way to interact with its services, helping them find answers quickly and easily. With future improvements, such as fine-tuning for domain-specific questions or incorporating advanced features, this chatbot has the potential to enhance customer satisfaction and streamline operations.\n",
    "\n",
    "Thank you for exploring this solution, and feel free to customize it further for your needs!\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6550821,
     "sourceId": 10585323,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
